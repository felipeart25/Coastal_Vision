{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.login(key=\"d42992a374fbc96ee65d1955f037e71d58e30f45\")\n",
    "wandb.init(project=\"THESIS\",\n",
    "    name=f\"hybrid_v1-{wandb.util.generate_id()}\",\n",
    "    config={\n",
    "    \"input_dim\": 1,\n",
    "    \"hidden_dims\": [128, 64, 64],\n",
    "    \"kernel_sizes\": [(5, 5),(5, 5),(5, 5)],\n",
    "    \"num_layers\": 3,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"MSELoss\"\n",
    "})\n",
    "\n",
    "# Load dataset (example: MNIST-like sequences)\n",
    "os.system('wget \"https://github.com/felipeart25/Coastal_Vision/raw/main/data/Data/mnist_test_seq.npy\" -O mnist_test_seq.npy')\n",
    "data = np.load(\"mnist_test_seq.npy\")  # Shape: (num_sequences, time_steps, channels, height, width)\n",
    "data = torch.tensor(data, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "data = data.unsqueeze(2)\n",
    "data = data.permute(1, 0, 2, 3, 4)  # Swap axes \n",
    "\n",
    "# Mock wave data (replace with real data)\n",
    "wave_data = torch.randn(len(data), 360, 1)  # 360 hours = 15 days\n",
    "\n",
    "# Print shape\n",
    "print(\"Original data shape:\", data.shape)  # Should be (num_sequences, time_steps, 1, height, width)\n",
    "\n",
    "# Split into train (70%), validation (15%), and test (15%)\n",
    "train_size = int(0.8 * len(data))  # 70% for training\n",
    "val_size = int(0.1 * len(data))   # 15% for validation\n",
    "test_size = len(data) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Prepare datasets\n",
    "# Input: first T-10 frames, Target: next 10 frames\n",
    "T = 20  # Number of input frames (T-10 for input, 10 for target)\n",
    "train_dataset = TensorDataset(train_data[:, :T-10], train_data[:, -10:])  # Input: T-10, Target: 10\n",
    "val_dataset = TensorDataset(val_data[:, :T-10], val_data[:, -10:])\n",
    "test_dataset = TensorDataset(test_data[:, :T-10], test_data[:, -10:])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = wandb.config.batch_size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify shapes\n",
    "for inputs, targets in train_loader:\n",
    "    print(\"Train Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Train Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "for inputs, targets in val_loader:\n",
    "    print(\"Validation Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Validation Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    print(\"Test Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Test Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ConvLSTM cell.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: int\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether to add bias or not.\n",
    "        \"\"\"\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,  # For the four gates\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_tensor: 4D tensor\n",
    "            Input tensor of shape (batch_size, input_dim, height, width)\n",
    "        cur_state: tuple\n",
    "            Current hidden and cell states (h_cur, c_cur)\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        h_next, c_next: next hidden and cell states\n",
    "        \"\"\"\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        # Concatenate along channel axis\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        \n",
    "        # Convolutional operation\n",
    "        combined_conv = self.conv(combined)\n",
    "        \n",
    "        # Split the combined output into the 4 gates\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        \n",
    "        # Apply gate activations\n",
    "        i = torch.sigmoid(cc_i)  # input gate\n",
    "        f = torch.sigmoid(cc_f)  # forget gate\n",
    "        o = torch.sigmoid(cc_o)  # output gate\n",
    "        g = torch.tanh(cc_g)     # cell gate\n",
    "        \n",
    "        # Update cell state and hidden state\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM module for sequence prediction with multiple layers and varying hidden dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, batch_first=True, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels in input\n",
    "        hidden_dims: list of ints\n",
    "            List of hidden dimensions for each layer\n",
    "        kernel_sizes: list of tuples\n",
    "            List of kernel sizes for each layer\n",
    "        num_layers: int\n",
    "            Number of LSTM layers stacked on each other\n",
    "        batch_first: bool\n",
    "            If True, dimension 0 is batch, dimension 1 is time, dimension 2 is channel.\n",
    "            If False, dimension 0 is time, dimension 1 is batch, dimension 2 is channel.\n",
    "        bias: bool\n",
    "            Whether to add bias or not\n",
    "        \"\"\"\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        assert len(hidden_dims) == num_layers, \"Length of hidden_dims must match num_layers\"\n",
    "        assert len(kernel_sizes) == num_layers, \"Length of kernel_sizes must match num_layers\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "\n",
    "        # Create a list of ConvLSTM cells\n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(cur_input_dim, self.hidden_dims[i], self.kernel_sizes[i], self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Size of the batch\n",
    "        image_size: tuple\n",
    "            Height and width of the feature maps\n",
    "        Returns:\n",
    "        -------\n",
    "        init_states: list\n",
    "            List of tuples (h, c) for each layer\n",
    "        \"\"\"\n",
    "        height, width = image_size\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            h = torch.zeros(batch_size, self.hidden_dims[i], height, width, device=self.cell_list[0].conv.weight.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_dims[i], height, width, device=self.cell_list[0].conv.weight.device)\n",
    "            init_states.append((h, c))\n",
    "        return init_states\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Forward pass through ConvLSTM layers.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_tensor: 5D tensor\n",
    "            Input of shape (batch_size, time, channels, height, width) if batch_first\n",
    "            or (time, batch_size, channels, height, width) otherwise\n",
    "        hidden_state: list of tuples\n",
    "            List of tuples (h, c) for each layer\n",
    "        Returns:\n",
    "        -------\n",
    "        layer_output_list: list\n",
    "            List of outputs from each layer\n",
    "        last_state_list: list\n",
    "            List of final states from each layer\n",
    "        \"\"\"\n",
    "        # Make sure we're working with batch first format\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Get dimensions\n",
    "        batch_size, seq_len, _, height, width = input_tensor.size()\n",
    "\n",
    "        # Initialize hidden states if none provided\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size, (height, width))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        # Process each sequence element\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                # Get input for this timestep\n",
    "                if layer_idx == 0:\n",
    "                    # For the first layer, input comes from the original input sequence\n",
    "                    x = input_tensor[:, t, :, :, :]\n",
    "                else:\n",
    "                    # For subsequent layers, input comes from the output of the previous layer\n",
    "                    x = layer_output_list[layer_idx - 1][:, t, :, :, :]\n",
    "\n",
    "                # Process through the ConvLSTM cell\n",
    "                h, c = self.cell_list[layer_idx](x, (h, c))\n",
    "\n",
    "                # Store output\n",
    "                output_inner.append(h)\n",
    "\n",
    "            # Stack outputs along time dimension\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append((h, c))\n",
    "\n",
    "        # Return outputs as needed\n",
    "        return layer_output_list[-1], last_state_list\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, lstm_hidden_size):\n",
    "        super(Predictor, self).__init__()\n",
    "        # Slow stream (ConvLSTM for spatial data)\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fast stream (LSTM for hourly wave data)\n",
    "        self.fast_lstm = nn.LSTM(\n",
    "            input_size=1,  # Wave height as scalar per timestep\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fusion layer: Combine ConvLSTM features + LSTM context\n",
    "        self.fusion_conv = nn.Conv2d(\n",
    "            in_channels=hidden_dims[-1] + lstm_hidden_size,\n",
    "            out_channels=input_dim,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, slow_input, fast_input, future_seq=10):\n",
    "        # Process slow stream (spatial data)\n",
    "        convlstm_output, convlstm_states = self.convlstm(slow_input)\n",
    "        convlstm_final = convlstm_output[:, -1]  # Last output feature map\n",
    "        \n",
    "        # Process fast stream (hourly wave data)\n",
    "        _, (h_n, _) = self.fast_lstm(fast_input)\n",
    "        lstm_context = h_n[-1].unsqueeze(-1).unsqueeze(-1)  # (batch, hidden, 1, 1)\n",
    "        \n",
    "        # Spatially broadcast LSTM context to match ConvLSTM's spatial dims\n",
    "        batch_size, _, height, width = convlstm_final.shape\n",
    "        lstm_context = lstm_context.expand(-1, -1, height, width)\n",
    "        \n",
    "        # Fuse features and predict future frames\n",
    "        combined = torch.cat([convlstm_final, lstm_context], dim=1)\n",
    "        predictions = self.fusion_conv(combined).unsqueeze(1)  # (batch, 1, C, H, W)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "    \n",
    "    # Track total time for the epoch\n",
    "    start_epoch = torch.cuda.Event(enable_timing=True)\n",
    "    end_epoch = torch.cuda.Event(enable_timing=True)\n",
    "    start_epoch.record()\n",
    "    for batch_idx, (slow_input, fast_input, target) in enumerate(train_loader):\n",
    "        slow_input = slow_input.to(device)\n",
    "        fast_input = fast_input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        start_iter = torch.cuda.Event(enable_timing=True)\n",
    "        end_iter = torch.cuda.Event(enable_timing=True)\n",
    "        start_iter.record()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(slow_input, fast_input)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end_iter.record()\n",
    "        torch.cuda.synchronize()\n",
    "        iteration_time = start_iter.elapsed_time(end_iter)/1000\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Log time to WandB\n",
    "        wandb.log({\"Batch Loss\": loss.item(), \"Iteration Time (s)\": iteration_time})\n",
    "        \n",
    "        if batch_idx % 100 == 0:  # Print every 100 batches\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Time per Iteration: {iteration_time:.4f}s\")\n",
    "\n",
    "    end_epoch.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    epoch_time = start_epoch.elapsed_time(end_epoch)/1000\n",
    "    avg_iteration_time = epoch_time / total_batches\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, avg_train_loss),  )\n",
    "    \n",
    "    # Log epoch train loss to WandB\n",
    "    wandb.log({\"Epoch Train Loss\": avg_train_loss, \"Epoch\": epoch, \"Avg Iteration Time (s)\": avg_iteration_time})\n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in val_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            \n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target_seq)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Log epoch validation loss to WandB\n",
    "    wandb.log({\"Epoch Validation Loss\": avg_val_loss, \"Epoch\": epoch})\n",
    "    \n",
    "    return avg_val_loss\n",
    "\n",
    "def visualize_prediction(model, test_loader, device, sample_idx=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample from the test set\n",
    "    for i, (input_seq, target_seq) in enumerate(test_loader):\n",
    "        if i == sample_idx:\n",
    "            break\n",
    "    \n",
    "    input_seq = input_seq.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "    \n",
    "    # Input sequence\n",
    "    for t in range(10):\n",
    "        axes[0, t].imshow(input_seq[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0, t].set_title(f'Input t={t}')\n",
    "        axes[0, t].axis('off')\n",
    "    \n",
    "    # Target sequence\n",
    "    for t in range(10):\n",
    "        axes[1, t].imshow(target_seq[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[1, t].set_title(f'Target t={t+10}')\n",
    "        axes[1, t].axis('off')\n",
    "    \n",
    "    # Predicted sequence\n",
    "    for t in range(10):\n",
    "        axes[2, t].imshow(output[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[2, t].set_title(f'Pred t={t+10}')\n",
    "        axes[2, t].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mnist_prediction.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the visualization to WandB\n",
    "    wandb.log({\"Predictions\": wandb.Image('mnist_prediction.png')})\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Trainable Parameters: {total_params:,}\")  # Format with commas\n",
    "    \n",
    "    # ðŸ”¹ Log to WandB\n",
    "    wandb.log({\"Total Parameters\": total_params})\n",
    "    \n",
    "    return total_params\n",
    "    \n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Hyperparameters (already logged in WandB init)\n",
    "    input_dim = wandb.config.input_dim\n",
    "    hidden_dims = wandb.config.hidden_dims\n",
    "    kernel_sizes = wandb.config.kernel_sizes\n",
    "    num_layers = wandb.config.num_layers\n",
    "    batch_size = wandb.config.batch_size\n",
    "    lstm_hidden_size = wandb.config.lstm_hidden_size\n",
    "    epochs = wandb.config.epochs\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    \n",
    "    # Create model\n",
    "    model = Predictor(input_dim=input_dim, hidden_dims=hidden_dims, kernel_sizes=kernel_sizes, num_layers=num_layers, lstm_hidden_size=lstm_hidden_size).to(device)\n",
    "    print(f\"Model is running with {model.convlstm.num_layers} layers.\")\n",
    "\n",
    "    # Count trainable parameters\n",
    "    total_params = count_parameters(model)\n",
    "    \n",
    "    # Log model architecture to WandB\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        val_loss = validate(model, test_loader, criterion, device, epoch)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Save model\n",
    "    model_path = \"convlstm_mnist.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    artifact = wandb.Artifact(name=\"Conv-LSTM\", type=\"model\")\n",
    "    artifact.add_file(model_path)\n",
    "    wandb.log_artifact(artifact)  # Log model checkpoint to WandB\n",
    "\n",
    "    wandb.run.log_code(\".\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_prediction(model, test_loader, device)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('loss_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log loss curves to WandB\n",
    "    wandb.log({\"Loss Curves\": wandb.Image('loss_curves.png')})\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
