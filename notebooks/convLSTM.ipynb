{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=\"convlstm-mnist\", config={\n",
    "    \"input_dim\": 1,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"kernel_size\": (3, 3),\n",
    "    \"num_layers\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 3,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"MSELoss\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (example: MNIST-like sequences)\n",
    "!wget \"https://github.com/felipeart25/Coastal_Vision/raw/main/data/Data/mnist_test_seq.npy\" -O mnist_test_seq.npy\n",
    "data = np.load(\"mnist_test_seq.npy\")  # Shape: (num_sequences, time_steps, channels, height, width)\n",
    "data = torch.tensor(data, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "data = data.unsqueeze(2)\n",
    "data = data.permute(1, 0, 2, 3, 4)  # Swap axes \n",
    "\n",
    "# Print shape\n",
    "print(\"Original data shape:\", data.shape)  # Should be (num_sequences, time_steps, 1, height, width)\n",
    "\n",
    "# Split into train (70%), validation (15%), and test (15%)\n",
    "train_size = int(0.8 * len(data))  # 70% for training\n",
    "val_size = int(0.1 * len(data))   # 15% for validation\n",
    "test_size = len(data) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Prepare datasets\n",
    "# Input: first T-10 frames, Target: next 10 frames\n",
    "T = 20  # Number of input frames (T-10 for input, 10 for target)\n",
    "train_dataset = TensorDataset(train_data[:, :T-10], train_data[:, -10:])  # Input: T-10, Target: 10\n",
    "val_dataset = TensorDataset(val_data[:, :T-10], val_data[:, -10:])\n",
    "test_dataset = TensorDataset(test_data[:, :T-10], test_data[:, -10:])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify shapes\n",
    "for inputs, targets in train_loader:\n",
    "    print(\"Train Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Train Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "for inputs, targets in val_loader:\n",
    "    print(\"Validation Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Validation Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    print(\"Test Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, H, W)\n",
    "    print(\"Test Targets shape:\", targets.shape)  # Should be (B, 10, 1, H, W)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE CONVLSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ConvLSTM cell.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: int\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether to add bias or not.\n",
    "        \"\"\"\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,  # For the four gates\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_tensor: 4D tensor\n",
    "            Input tensor of shape (batch_size, input_dim, height, width)\n",
    "        cur_state: tuple\n",
    "            Current hidden and cell states (h_cur, c_cur)\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        h_next, c_next: next hidden and cell states\n",
    "        \"\"\"\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        # Concatenate along channel axis\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        \n",
    "        # Convolutional operation\n",
    "        combined_conv = self.conv(combined)\n",
    "        \n",
    "        # Split the combined output into the 4 gates\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        \n",
    "        # Apply gate activations\n",
    "        i = torch.sigmoid(cc_i)  # input gate\n",
    "        f = torch.sigmoid(cc_f)  # forget gate\n",
    "        o = torch.sigmoid(cc_o)  # output gate\n",
    "        g = torch.tanh(cc_g)     # cell gate\n",
    "        \n",
    "        # Update cell state and hidden state\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM module for sequence prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True, bias=True):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels in input\n",
    "        hidden_dim: int\n",
    "            Number of hidden channels\n",
    "        kernel_size: int\n",
    "            Size of kernel in convolutions\n",
    "        num_layers: int\n",
    "            Number of LSTM layers stacked on each other\n",
    "        batch_first: bool\n",
    "            If True, dimension 0 is batch, dimension 1 is time, dimension 2 is channel.\n",
    "            If False, dimension 0 is time, dimension 1 is batch, dimension 2 is channel.\n",
    "        bias: bool\n",
    "            Whether to add bias or not\n",
    "        \"\"\"\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        \n",
    "        # Create a list of ConvLSTM cells\n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim\n",
    "            cell_list.append(ConvLSTMCell(cur_input_dim, self.hidden_dim, self.kernel_size, self.bias))\n",
    "        \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Size of the batch\n",
    "        image_size: tuple\n",
    "            Height and width of the feature maps\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        init_states: list\n",
    "            List of tuples (h, c) for each layer\n",
    "        \"\"\"\n",
    "        height, width = image_size\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.cell_list[0].conv.weight.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.cell_list[0].conv.weight.device)\n",
    "            init_states.append((h, c))\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Forward pass through ConvLSTM layers.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_tensor: 5D tensor\n",
    "            Input of shape (batch_size, time, channels, height, width) if batch_first\n",
    "            or (time, batch_size, channels, height, width) otherwise\n",
    "        hidden_state: list of tuples\n",
    "            List of tuples (h, c) for each layer\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        layer_output_list: list\n",
    "            List of outputs from each layer\n",
    "        last_state_list: list\n",
    "            List of final states from each layer\n",
    "        \"\"\"\n",
    "        # Make sure we're working with batch first format\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "            \n",
    "        # Get dimensions\n",
    "        batch_size, seq_len, _, height, width = input_tensor.size()\n",
    "        \n",
    "        # Initialize hidden states if none provided\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size, (height, width))\n",
    "            \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        # Process each sequence element\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                # Get input for this timestep\n",
    "                if layer_idx == 0:\n",
    "                    # For the first layer, input comes from the original input sequence\n",
    "                    x = input_tensor[:, t, :, :, :]\n",
    "                else:\n",
    "                    # For subsequent layers, input comes from the output of the previous layer\n",
    "                    x = layer_output_list[layer_idx-1][:, t, :, :, :]\n",
    "                    \n",
    "                # Process through the ConvLSTM cell\n",
    "                h, c = self.cell_list[layer_idx](x, (h, c))\n",
    "                \n",
    "                # Store output\n",
    "                output_inner.append(h)\n",
    "                \n",
    "            # Stack outputs along time dimension\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append((h, c))\n",
    "            \n",
    "        # Return outputs as needed\n",
    "        return layer_output_list[-1], last_state_list\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.convlstm = ConvLSTM(input_dim=input_dim,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                kernel_size=kernel_size,\n",
    "                                num_layers=num_layers)\n",
    "        self.conv_output = nn.Conv2d(hidden_dim, input_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, future_seq=10):\n",
    "        # Process input sequence\n",
    "        _, lstm_states = self.convlstm(x)\n",
    "        \n",
    "        # Generate future predictions\n",
    "        current_input = x[:, -1]  # Last input frame\n",
    "        outputs = []\n",
    "        \n",
    "        hidden_state = lstm_states\n",
    "        \n",
    "        for _ in range(future_seq):\n",
    "            # Reshape for input to ConvLSTM cell\n",
    "            current_input = current_input.unsqueeze(1)  # Add time dimension\n",
    "            \n",
    "            # Forward pass through ConvLSTM\n",
    "            lstm_output, hidden_state = self.convlstm(current_input, hidden_state)\n",
    "            \n",
    "            # Generate prediction\n",
    "            current_input = self.conv_output(lstm_output[:, 0])\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs.append(current_input.unsqueeze(1))\n",
    "        \n",
    "        # Concatenate all predictions\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (input_seq, future_seq) in enumerate(train_loader):\n",
    "        input_seq, future_seq = input_seq.to(device), future_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "\n",
    "        loss = criterion(output, future_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Log batch loss to WandB\n",
    "        wandb.log({\"Batch Loss\": loss.item()})\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, avg_train_loss))\n",
    "    \n",
    "    # Log epoch train loss to WandB\n",
    "    wandb.log({\"Epoch Train Loss\": avg_train_loss, \"Epoch\": epoch})\n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in val_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            \n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target_seq)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Log epoch validation loss to WandB\n",
    "    wandb.log({\"Epoch Validation Loss\": avg_val_loss, \"Epoch\": epoch})\n",
    "    \n",
    "    return avg_val_loss\n",
    "\n",
    "def visualize_prediction(model, test_loader, device, sample_idx=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample from the test set\n",
    "    for i, (input_seq, target_seq) in enumerate(test_loader):\n",
    "        if i == sample_idx:\n",
    "            break\n",
    "    \n",
    "    input_seq = input_seq.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "    \n",
    "    # Input sequence\n",
    "    for t in range(10):\n",
    "        axes[0, t].imshow(input_seq[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0, t].set_title(f'Input t={t}')\n",
    "        axes[0, t].axis('off')\n",
    "    \n",
    "    # Target sequence\n",
    "    for t in range(10):\n",
    "        axes[1, t].imshow(target_seq[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[1, t].set_title(f'Target t={t+10}')\n",
    "        axes[1, t].axis('off')\n",
    "    \n",
    "    # Predicted sequence\n",
    "    for t in range(10):\n",
    "        axes[2, t].imshow(output[0, t, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[2, t].set_title(f'Pred t={t+10}')\n",
    "        axes[2, t].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mnist_prediction.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the visualization to WandB\n",
    "    wandb.log({\"Predictions\": wandb.Image('mnist_prediction.png')})\n",
    "    \n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Hyperparameters (already logged in WandB init)\n",
    "    input_dim = wandb.config.input_dim\n",
    "    hidden_dim = wandb.config.hidden_dim\n",
    "    kernel_size = wandb.config.kernel_size\n",
    "    num_layers = wandb.config.num_layers\n",
    "    batch_size = wandb.config.batch_size\n",
    "    epochs = wandb.config.epochs\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    \n",
    "    # Create model\n",
    "    model = Predictor(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers).to(device)\n",
    "    \n",
    "    # Log model architecture to WandB\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        val_loss = validate(model, test_loader, criterion, device, epoch)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'convlstm_mnist.pth')\n",
    "    wandb.save('convlstm_mnist.pth')  # Log model checkpoint to WandB\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_prediction(model, test_loader, device)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('loss_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log loss curves to WandB\n",
    "    wandb.log({\"Loss Curves\": wandb.Image('loss_curves.png')})\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
