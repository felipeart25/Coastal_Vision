{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of a ConvLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ConvLSTM Model\n",
    "\n",
    "The ConvLSTMCell class implements a single ConvLSTM unit, which processes input frame sequences one time step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 future step prediction\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias): #input_dim = Number of channels in input, hidden_dim = Number of channels in hidden state, kernel_size = Size of the convolutional kernel, bias = If False, then the layer does not use bias weights b_ih and b_hh. Default\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # To compute all gates at once\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # Unpack the current hidden and cell states\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1) # Concatenate input with previous hidden state\n",
    "        combined_conv = self.conv(combined) # Apply convolutional layer\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) # The output of the convolution is split into four equal parts, each corresponding to one of the LSTM gates.\n",
    "        i = torch.sigmoid(cc_i) # Input gate\n",
    "        f = torch.sigmoid(cc_f) # Forget gate\n",
    "        o = torch.sigmoid(cc_o) # Output gate\n",
    "        g = torch.tanh(cc_g) # Candidate cell state\n",
    "        c_next = f * c_cur + i * g # Compute the next cell state\n",
    "        h_next = o * torch.tanh(c_next) # Compute the next hidden state\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size): # Creates tensors filled with zeros for both h_0 and c_0\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device), \n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module): # This class stacks multiple ConvLSTMCell layers to process entire sequences.\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False): # num_layers = Number of layers in the network\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # Allows specifying different kernel sizes and hidden dimensions per layer.\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): #Creates a list of ConvLSTMCell layers. The first layer receives input_dim, while subsequent layers receive the previous layer’s hidden_dim.\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first: #Ensures the input shape follows (batch, seq, channels, height, width).\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w)) #If no initial state is given, it initializes zeros.\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx] # Get initial hidden and cell states for the current layer\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size): #Calls init_hidden for each layer, ensuring they are properly initialized.\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers): #Ensures that the kernel size and hidden dimensions are lists of length num_layers.\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10 future step prediction\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias): #input_dim = Number of channels in input, hidden_dim = Number of channels in hidden state, kernel_size = Size of the convolutional kernel, bias = If False, then the layer does not use bias weights b_ih and b_hh. Default\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim, # To compute all gates at once\n",
    "            padding=self.padding,\n",
    "            kernel_size=self.kernel_size,\n",
    "            bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # Unpack the current hidden and cell states\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1) # Concatenate input with previous hidden state\n",
    "        combined_conv = self.conv(combined) # Apply convolutional layer\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) # The output of the convolution is split into four equal parts, each corresponding to one of the LSTM gates.\n",
    "\n",
    "        i = torch.sigmoid(cc_i) # Input gate\n",
    "        f = torch.sigmoid(cc_f) # Forget gate\n",
    "        o = torch.sigmoid(cc_o) # Output gate\n",
    "        g = torch.tanh(cc_g) # Candidate cell state\n",
    "        c_next = f * c_cur + i * g # Compute the next cell state\n",
    "        h_next = o * torch.tanh(c_next) # Compute the next hidden state\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size): # Creates tensors filled with zeros for both h_0 and c_0\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device), \n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module): # This class stacks multiple ConvLSTMCell layers to process entire sequences.\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True, return_all_layers=False): # num_layers = Number of layers in the network\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # Allows specifying different kernel sizes and hidden dimensions per layer.\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): #Creates a list of ConvLSTMCell layers. The first layer receives input_dim, while subsequent layers receive the previous layer’s hidden_dim.\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "            \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(in_channels=self.hidden_dim[-1],  # Hidden dim of the last layer\n",
    "                                    out_channels=input_dim,          # Output channels (e.g., 1 for grayscale)\n",
    "                                    kernel_size=1,                   # 1x1 convolution\n",
    "                                    padding=0,\n",
    "                                    bias=bias)\n",
    "\n",
    "    def forward(self, input_tensor, future_frames=10):\n",
    "      #print(\"Input tensor shape:\", input_tensor.shape)  # Debugging statement\n",
    "      if not self.batch_first:\n",
    "          input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "      b, _, _, h, w = input_tensor.size()\n",
    "      #print(\"Batch size:\", b, \"Time steps:\", _, \"Height:\", h, \"Width:\", w)\n",
    "\n",
    "      # Initialize hidden states\n",
    "      hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "      # Process input sequence\n",
    "      layer_output_list = []\n",
    "      last_state_list = []\n",
    "      cur_layer_input = input_tensor\n",
    "\n",
    "      for layer_idx in range(self.num_layers):\n",
    "          h, c = hidden_state[layer_idx]\n",
    "          output_inner = []\n",
    "          for t in range(input_tensor.size(1)):\n",
    "              h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "              output_inner.append(h)\n",
    "          layer_output = torch.stack(output_inner, dim=1)\n",
    "          cur_layer_input = layer_output\n",
    "          layer_output_list.append(layer_output)\n",
    "          last_state_list.append([h, c])\n",
    "\n",
    "      # Predict future frames\n",
    "      outputs = []\n",
    "      last_output = layer_output[:, -1]\n",
    "      #print(\"Last output shape (before prediction):\", last_output.shape)\n",
    "      for t in range(future_frames):\n",
    "          last_output, c = self.cell_list[-1](input_tensor=last_output, cur_state=[last_output, c])\n",
    "          #print(\"Last output shape (during prediction):\", last_output.shape)\n",
    "          outputs.append(self.conv_out(last_output))\n",
    "      outputs = torch.stack(outputs, dim=1)\n",
    "      #print(\"Outputs shape:\", outputs.shape)\n",
    "\n",
    "      return outputs\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size): #Calls init_hidden for each layer, ensuring they are properly initialized.\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers): #Ensures that the kernel size and hidden dimensions are lists of length num_layers.\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Including Self-Attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=8):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable scaling factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        N = height * width  # Number of spatial locations\n",
    "\n",
    "        # Map input to Query, Key, and Value\n",
    "        query = self.query_conv(x).view(batch_size, -1, N).permute(0, 2, 1)  # (B, N, C')\n",
    "        key = self.key_conv(x).view(batch_size, -1, N)                      # (B, C', N)\n",
    "        value = self.value_conv(x).view(batch_size, -1, N)                  # (B, C, N)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        energy = torch.bmm(query, key)  # (B, N, N)\n",
    "        attention = F.softmax(energy, dim=-1)  # Normalize along columns\n",
    "\n",
    "        # Weighted aggregation\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))  # (B, C, N)\n",
    "        out = out.view(batch_size, channels, height, width)  # Reshape back to spatial dimensions\n",
    "\n",
    "        # Add residual connection and scale\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ConvLSTMCellWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCellWithAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # Convolutional layers for gates\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size[0] // 2,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "        # Self-attention modules\n",
    "        self.input_attention = SelfAttention(input_dim)\n",
    "        self.hidden_attention = SelfAttention(hidden_dim)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        # Apply self-attention to input and hidden state\n",
    "        input_tensor = self.input_attention(input_tensor)\n",
    "        h_cur = self.hidden_attention(h_cur)\n",
    "\n",
    "        # Concatenate input and hidden state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "\n",
    "        # Compute gates\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        # Update cell and hidden states\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (\n",
    "            torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "            torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        )\n",
    "    \n",
    "class ConvLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True):\n",
    "        super(ConvLSTMWithAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Stack multiple ConvLSTM cells with attention\n",
    "        cell_list = []\n",
    "        for i in range(num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else hidden_dim\n",
    "            cell_list.append(ConvLSTMCellWithAttention(cur_input_dim, hidden_dim, kernel_size))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "        # Output layer\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, input_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, input_tensor, future_frames=0):\n",
    "        if self.batch_first:\n",
    "            input_tensor = input_tensor.permute(0, 1, 4, 2, 3)  # (B, T, H, W, C) -> (B, T, C, H, W)\n",
    "\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden_state = [cell.init_hidden(b, (h, w)) for cell in self.cell_list]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            cur_layer_input = input_tensor[:, t]\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                h, c = self.cell_list[layer_idx](cur_layer_input, hidden_state[layer_idx])\n",
    "                hidden_state[layer_idx] = (h, c)\n",
    "                cur_layer_input = h\n",
    "            outputs.append(h)\n",
    "\n",
    "        # Autoregressive prediction for future frames\n",
    "        for t in range(future_frames):\n",
    "            cur_layer_input = outputs[-1]\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                h, c = self.cell_list[layer_idx](cur_layer_input, hidden_state[layer_idx])\n",
    "                hidden_state[layer_idx] = (h, c)\n",
    "                cur_layer_input = h\n",
    "            outputs.append(h)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # (B, T, C, H, W)\n",
    "        outputs = self.conv_out(outputs.view(-1, self.hidden_dim, h, w)).view(b, -1, 1, h, w)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist_test_seq.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset (example: MNIST-like sequences)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/felipeart25/Coastal_Vision/raw/main/data/Data/mnist_test_seq.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -O mnist_test_seq.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist_test_seq.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (num_sequences, time_steps, channels, height, width)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize to [0, 1]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Split into train and test\u001b[39;00m\n",
      "File \u001b[1;32md:\\My Documents\\Thesis\\Coastal_vision\\venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_test_seq.npy'"
     ]
    }
   ],
   "source": [
    "# Load dataset (example: MNIST-like sequences)\n",
    "!wget \"https://github.com/felipeart25/Coastal_Vision/raw/main/data/Data/mnist_test_seq.npy\" -O mnist_test_seq.npy\n",
    "data = np.load(\"mnist_test_seq.npy\")  # Shape: (num_sequences, time_steps, channels, height, width)\n",
    "data = torch.tensor(data, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "data = data.unsqueeze(2)\n",
    "data = data.permute(1, 0, 2, 3, 4)  # Swap axes \n",
    "\n",
    "#print shape\n",
    "print(data.shape)\n",
    "# Split into train and test\n",
    "train_data, test_data = data[:8000], data[8000:]\n",
    "\n",
    "# Input: first T-10 frames, Target: next 10 frames\n",
    "train_dataset = TensorDataset(train_data[:, :10], train_data[:, -10:])  # Input: T-10, Target: 10\n",
    "print(train_data.shape)\n",
    "test_dataset = TensorDataset(test_data[:, :10], test_data[:, -10:])\n",
    "print(test_data.shape)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    print(\"Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, 64, 64)\n",
    "    print(\"Targets shape:\", targets.shape)  # Should be (B, 10, 1, 64, 64)\n",
    "    break\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    print(\"Inputs shape:\", inputs.shape)  # Should be (B, T-10, 1, 64, 64)\n",
    "    print(\"Targets shape:\", targets.shape)  # Should be (B, 10, 1, 64, 64)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training\n",
    "\n",
    "This code initializes and trains a ConvLSTM model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvLSTM(input_dim=1, hidden_dim=64, kernel_size=(3, 3), num_layers=2, batch_first=True).to(device)\n",
    "criterion = nn.MSELoss() # MSE is commonly used for regression tasks, which is suitable for predicting pixel intensities in images\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Uses the Adam optimizer with a learning rate (lr) of 0.001 to update model weights\n",
    "print(\"Input tensor shape:\", inputs.shape)\n",
    "# Training loop The model is trained for 10 epochs. Each epoch means the model sees the entire training dataset once.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    total_loss = 0 #Initializes a variable to accumulate the total loss for the epoch\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() #Resets the gradients before backpropagation. By default, gradients accumulate in PyTorch, so this step prevents incorrect updates.\n",
    "        outputs = model(inputs, future_frames=10)  # Predict 10 frames\n",
    "        loss = criterion(outputs, targets)  # Compute loss across all 10 frames\n",
    "        loss.backward() #Computes gradients of the loss with respect to model parameters\n",
    "        optimizer.step() #Updates the model’s parameters using the gradients. This step is where the actual learning happens.\n",
    "        total_loss += loss.item() #Accumulates the loss for the current batch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def validate(model, loader, criterion, device, future_frames=10):\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, total_ssim = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs, future_frames=future_frames)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mae = torch.abs(outputs - targets).mean().item()\n",
    "            rmse = torch.sqrt(((outputs - targets) ** 2).mean()).item()\n",
    "            \n",
    "            # Convert to numpy for SSIM\n",
    "            output_np = outputs.cpu().numpy()\n",
    "            target_np = targets.cpu().numpy()\n",
    "            ssim_score = ssim(output_np, target_np, data_range=1.0, multichannel=True)\n",
    "            \n",
    "            total_mae += mae\n",
    "            total_rmse += rmse\n",
    "            total_ssim += ssim_score\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    avg_rmse = total_rmse / len(loader)\n",
    "    avg_ssim = total_ssim / len(loader)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, RMSE: {avg_rmse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
    "    return avg_loss, avg_mae, avg_rmse, avg_ssim\n",
    "\n",
    "# Validate after each epoch\n",
    "validate(model, test_loader, criterion, device, future_frames=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    outputs = model(inputs, future_frames=10).cpu().numpy()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(targets[0, i].cpu().squeeze(), cmap=\"gray\")\n",
    "    axes[0, i].set_title(\"Ground Truth\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(outputs[0, i].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].set_title(\"Prediction\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
