{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of a ConvLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ConvLSTM Model\n",
    "\n",
    "The ConvLSTMCell class implements a single ConvLSTM unit, which processes input frame sequences one time step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias): #input_dim = Number of channels in input, hidden_dim = Number of channels in hidden state, kernel_size = Size of the convolutional kernel, bias = If False, then the layer does not use bias weights b_ih and b_hh. Default\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # To compute all gates at once\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # Unpack the current hidden and cell states\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1) # Concatenate input with previous hidden state\n",
    "        combined_conv = self.conv(combined) # Apply convolutional layer\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) # The output of the convolution is split into four equal parts, each corresponding to one of the LSTM gates.\n",
    "        i = torch.sigmoid(cc_i) # Input gate\n",
    "        f = torch.sigmoid(cc_f) # Forget gate\n",
    "        o = torch.sigmoid(cc_o) # Output gate\n",
    "        g = torch.tanh(cc_g) # Candidate cell state\n",
    "        c_next = f * c_cur + i * g # Compute the next cell state\n",
    "        h_next = o * torch.tanh(c_next) # Compute the next hidden state\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size): # Creates tensors filled with zeros for both h_0 and c_0\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device), \n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module): # This class stacks multiple ConvLSTMCell layers to process entire sequences.\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False): # num_layers = Number of layers in the network\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # Allows specifying different kernel sizes and hidden dimensions per layer.\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): #Creates a list of ConvLSTMCell layers. The first layer receives input_dim, while subsequent layers receive the previous layer’s hidden_dim.\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first: #Ensures the input shape follows (batch, seq, channels, height, width).\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w)) #If no initial state is given, it initializes zeros.\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx] # Get initial hidden and cell states for the current layer\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size): #Calls init_hidden for each layer, ensuring they are properly initialized.\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers): #Ensures that the kernel size and hidden dimensions are lists of length num_layers.\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (example: MNIST-like sequences)\n",
    "data = np.load(\"path_to_your_data.npy\")  # Shape: (num_sequences, time_steps, channels, height, width)\n",
    "data = torch.tensor(data, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Split into train and test\n",
    "train_data, test_data = data[:800], data[800:]\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(train_data[:, :-1], train_data[:, -1])  # Input: first T-1 frames, Target: last frame\n",
    "test_dataset = TensorDataset(test_data[:, :-1], test_data[:, -1])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training\n",
    "\n",
    "This code initializes and trains a ConvLSTM model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvLSTM(input_dim=1, hidden_dim=64, kernel_size=(3, 3), num_layers=2, batch_first=True).to(device)\n",
    "criterion = nn.MSELoss() # MSE is commonly used for regression tasks, which is suitable for predicting pixel intensities in images\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Uses the Adam optimizer with a learning rate (lr) of 0.001 to update model weights\n",
    "\n",
    "# Training loop The model is trained for 10 epochs. Each epoch means the model sees the entire training dataset once.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    total_loss = 0 #Initializes a variable to accumulate the total loss for the epoch\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() #Resets the gradients before backpropagation. By default, gradients accumulate in PyTorch, so this step prevents incorrect updates.\n",
    "        outputs, _ = model(inputs) #Passes the input sequence through the model to get the output\n",
    "        outputs = outputs[0][:, -1]  # Take the last output frame\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward() #Computes gradients of the loss with respect to model parameters\n",
    "        optimizer.step() #Updates the model’s parameters using the gradients. This step is where the actual learning happens.\n",
    "        total_loss += loss.item() #Accumulates the loss for the current batch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, total_ssim = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs[0][:, -1]\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mae = torch.abs(outputs - targets).mean().item()\n",
    "            rmse = torch.sqrt(((outputs - targets) ** 2).mean()).item()\n",
    "            \n",
    "            # Convert to numpy for SSIM\n",
    "            output_np = outputs.cpu().numpy()\n",
    "            target_np = targets.cpu().numpy()\n",
    "            ssim_score = ssim(output_np, target_np, data_range=1.0, multichannel=True)\n",
    "            \n",
    "            total_mae += mae\n",
    "            total_rmse += rmse\n",
    "            total_ssim += ssim_score\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    avg_rmse = total_rmse / len(loader)\n",
    "    avg_ssim = total_ssim / len(loader)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, RMSE: {avg_rmse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
    "    return avg_loss, avg_mae, avg_rmse, avg_ssim\n",
    "\n",
    "# Validate after each epoch\n",
    "validate(model, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
